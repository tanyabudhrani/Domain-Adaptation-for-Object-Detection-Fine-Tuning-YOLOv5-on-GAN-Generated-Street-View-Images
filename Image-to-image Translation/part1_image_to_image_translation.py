# -*- coding: utf-8 -*-
"""Part1_image-to-image_translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQHSUUwJSVEPlZl-2L6NDFRaIagCMGKh
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms.functional as TF
import torchvision.models as models
from skimage.metrics import peak_signal_noise_ratio as psnr_metric, structural_similarity as ssim_metric
from torch.nn.utils import spectral_norm
import numpy as np
from PIL import Image
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ================================
# 1. Dataset Loader
# ================================
class CustomPairedDataset(Dataset):
    """Loads paired images from a given directory structure."""

    def __init__(self, root_dir, split=None, transform=None):
        self.transform = transform
        self.paired_paths = []

        target_dir = os.path.join(root_dir, split) if split else root_dir
        semantic_dir = os.path.join(target_dir, 'camera_images_semantic_front')
        real_dir = os.path.join(target_dir, 'camera_images_real_front')

        if not os.path.isdir(semantic_dir) or not os.path.isdir(real_dir):
            print(f"Missing expected subdirectories in {target_dir}")
        else:
            for filename in sorted(os.listdir(semantic_dir)):
                semantic_path = os.path.join(semantic_dir, filename)
                real_path = os.path.join(real_dir, filename)
                if os.path.exists(semantic_path) and os.path.exists(real_path):
                    self.paired_paths.append((semantic_path, real_path))

        print(f"Total paired images found: {len(self.paired_paths)}")

    def __len__(self):
        return len(self.paired_paths)

    def __getitem__(self, idx):
        semantic_path, real_path = self.paired_paths[idx]
        semantic_img = Image.open(semantic_path).convert("RGB")
        real_img = Image.open(real_path).convert("RGB")

        if self.transform:
            semantic_img = self.transform(semantic_img)
            real_img = self.transform(real_img)

        return semantic_img, real_img

# ================================
# 2. U-Net Generator
# ================================
class UNet(nn.Module):
    """Extended U-Net with 6 downscaling/upscaling layers for image-to-image translation."""

    def __init__(self, input_nc=3, output_nc=3, ngf=64):
        super(UNet, self).__init__()

        # Encoder (Downsampling)
        # Input (576x768) -> e1 (288x384) -> e2 (144x192) -> e3 (72x96) -> e4 (36x48) -> e5 (18x24) -> e6 (9x12)
        self.enc1 = self.conv_block(input_nc, ngf)            # output: ngf, 288x384
        self.enc2 = self.conv_block(ngf, ngf * 2)             # output: ngf*2, 144x192
        self.enc3 = self.conv_block(ngf * 2, ngf * 4)         # output: ngf*4, 72x96
        self.enc4 = self.conv_block(ngf * 4, ngf * 8)         # output: ngf*8, 36x48
        self.enc5 = self.conv_block(ngf * 8, ngf * 16)        # output: ngf*16, 18x24
        self.enc6 = self.conv_block(ngf * 16, ngf * 32)       # output: ngf*32, 9x12

        # Bottleneck (no further downsampling, retains 9x12 resolution)
        self.bottleneck = self.conv_block_no_downscale(ngf * 32, ngf * 32)

        # Decoder (Upsampling with Skip Connections)
        # Upsamples: 9x12 -> 18x24, 18x24 -> 36x48, 36x48 -> 72x96, 72x96 -> 144x192, 144x192 -> 288x384, 288x384 -> 576x768
        self.dec6 = nn.ConvTranspose2d(ngf * 32, ngf * 16, kernel_size=4, stride=2, padding=1)  # 9->18
        self.dec5 = nn.ConvTranspose2d(ngf * 16, ngf * 8, kernel_size=4, stride=2, padding=1)   # 18->36
        self.dec4 = nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1)    # 36->72
        self.dec3 = nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1)    # 72->144
        self.dec2 = nn.ConvTranspose2d(ngf * 2, ngf, kernel_size=4, stride=2, padding=1)        # 144->288
        self.dec1 = nn.ConvTranspose2d(ngf, output_nc, kernel_size=4, stride=2, padding=1)      # 288->576

    def forward(self, x):
        # Encoder: Downsampling stages
        e1 = self.enc1(x)   # 288x384, channels: ngf
        e2 = self.enc2(e1)  # 144x192, channels: ngf*2
        e3 = self.enc3(e2)  # 72x96, channels: ngf*4
        e4 = self.enc4(e3)  # 36x48, channels: ngf*8
        e5 = self.enc5(e4)  # 18x24, channels: ngf*16
        e6 = self.enc6(e5)  # 9x12, channels: ngf*32

        # Bottleneck
        b = self.bottleneck(e6)  # 9x12, channels: ngf*32

        # Decoder: Upsampling with Skip Connections
        d6 = self.dec6(b) + e5     # Upsample 9x12 -> 18x24, add skip from e5
        d5 = self.dec5(d6) + e4    # 18x24 -> 36x48, add skip from e4
        d4 = self.dec4(d5) + e3    # 36x48 -> 72x96, add skip from e3
        d3 = self.dec3(d4) + e2    # 72x96 -> 144x192, add skip from e2
        d2 = self.dec2(d3) + e1    # 144x192 -> 288x384, add skip from e1
        d1 = self.dec1(d2)         # 288x384 -> 576x768, final output layer

        # Ensure exact output size
        d1 = F.interpolate(d1, size=(576, 768), mode="bilinear", align_corners=False)
        return torch.tanh(d1)

# ================================
# 3. PatchGAN Discriminator
# ================================

class PatchGANDiscriminator(nn.Module):
    """Patch-based discriminator with spectral normalization and dropout."""

    def __init__(self, input_nc=3, ndf=128, n_layers=6, norm_layer=nn.InstanceNorm2d, dropout_rate=0.1):
        super(PatchGANDiscriminator, self).__init__()
        sequence = [
            spectral_norm(nn.Conv2d(input_nc * 2, ndf, kernel_size=4, stride=2, padding=1)),
            nn.LeakyReLU(0.2, True)
        ]
        nf_mult = 1
        for n in range(1, n_layers):
            nf_mult_prev = nf_mult
            nf_mult = min(2 ** n, 8)
            sequence += [
                spectral_norm(nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=4, stride=2, padding=1, bias=False)),
                norm_layer(ndf * nf_mult),
                nn.LeakyReLU(0.2, True),
                nn.Dropout(dropout_rate)
            ]
        sequence += [spectral_norm(nn.Conv2d(ndf * nf_mult, 1, kernel_size=4, stride=1, padding=1))]
        self.model = nn.Sequential(*sequence)

    def forward(self, input):
        return self.model(input)

class VGGFeatureExtractor(nn.Module):
    def __init__(self, layers=['conv1_2', 'conv2_2', 'conv3_3']):
        super(VGGFeatureExtractor, self).__init__()
        # Load the pre-trained VGG16 model
        vgg = models.vgg16(pretrained=True).features

        # Freeze the parameters so they are not updated during training
        for param in vgg.parameters():
            param.requires_grad = False

        self.vgg_layers = vgg
        self.selected_layers = layers

    def forward(self, x):
        features = {}
        for name, layer in self.vgg_layers._modules.items():
            x = layer(x)
            # Capture features at specified layers
            if name == '3' and 'conv1_2' in self.selected_layers:
                features['conv1_2'] = x
            elif name == '8' and 'conv2_2' in self.selected_layers:
                features['conv2_2'] = x
            elif name == '17' and 'conv3_3' in self.selected_layers:
                features['conv3_3'] = x
        return features

# ================================
# 4. Edge-Preserving Loss Function
# ================================

def laplacian_filter(img):
    """Converts image to grayscale and applies Laplacian filter."""
    grayscale_img = TF.rgb_to_grayscale(img, num_output_channels=1)
    laplacian_kernel = torch.tensor([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
    laplacian_kernel = laplacian_kernel.to(img.device)
    edge_map = F.conv2d(grayscale_img, laplacian_kernel, padding=1)
    return edge_map


def edge_preserving_loss(real_img, fake_img):
    """Computes edge-preserving loss based on Laplacian edge differences."""
    real_edges = laplacian_filter(real_img)
    fake_edges = laplacian_filter(fake_img)
    return torch.mean(torch.abs(real_edges - fake_edges))

# ================================
# 5. Training Function with Edge-Preserving Loss and Learning Rate Scheduling
# ================================

def train():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_epochs, lr, beta1, batch_size = 10, 0.0002, 0.5, 4
    target_size = (576, 768)

    train_path = "/content/drive/MyDrive/reference_images/Path_1"
    val_path = "/content/drive/MyDrive/reference_images/Path_3"

    transform = transforms.Compose([
        transforms.Resize(target_size),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    train_loader = DataLoader(CustomPairedDataset(train_path, transform=transform), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(CustomPairedDataset(val_path, transform=transform), batch_size=1, shuffle=False)

    # Initialize models
    netG = UNet().to(device)
    netD = PatchGANDiscriminator().to(device)

    # Loss functions
    criterion_GAN = nn.BCEWithLogitsLoss()
    criterion_L1 = nn.L1Loss()

    # Optimizers
    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))
    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))

    # Learning rate schedulers: Exponential decay
    schedulerG = torch.optim.lr_scheduler.ExponentialLR(optimizerG, gamma=0.99)
    schedulerD = torch.optim.lr_scheduler.ExponentialLR(optimizerD, gamma=0.99)

    # Instantiate the VGG feature extractor for perceptual loss
    vgg_extractor = VGGFeatureExtractor().to(device)
    lambda_perceptual = 10
    lambda_edge = 5

    print("Starting Training ...")
    for epoch in range(num_epochs):
        for i, (input_img, target_img) in enumerate(train_loader):
            input_img, target_img = input_img.to(device), target_img.to(device)

            ############################
            # (1) Update D network with Label Smoothing
            ############################
            optimizerD.zero_grad()

            # Generate fake image from input using U-Net
            fake_img = netG(input_img)
            real_pair = torch.cat((input_img, target_img), 1)
            fake_pair = torch.cat((input_img, fake_img), 1)

            # Compute discriminator outputs
            pred_real = netD(real_pair)
            pred_fake = netD(fake_pair.detach())

            # Create label tensors with label smoothing for real images
            label_real = torch.ones_like(pred_real) * 0.9  # Smoothed real labels
            label_fake = torch.zeros_like(pred_fake)

            # Calculate discriminator loss and update network
            loss_D = 0.5 * (criterion_GAN(pred_real, label_real) + criterion_GAN(pred_fake, label_fake))
            loss_D.backward()
            optimizerD.step()

            ############################
            # (2) Update G network
            ############################
            optimizerG.zero_grad()
            fake_img = netG(input_img)  # Recompute fake image for generator update
            fake_pair = torch.cat((input_img, fake_img), 1)
            pred_fake_for_G = netD(fake_pair)

            # Adversarial loss for the generator
            loss_G_adv = criterion_GAN(pred_fake_for_G, torch.ones_like(pred_fake_for_G))
            # L1 reconstruction loss
            loss_G_L1 = 100 * criterion_L1(fake_img, target_img)

            # Compute perceptual loss via VGG features
            vgg_fake = vgg_extractor(fake_img)
            vgg_real = vgg_extractor(target_img)
            loss_perceptual = 0
            for layer in vgg_fake.keys():
                loss_perceptual += torch.mean(torch.abs(vgg_fake[layer] - vgg_real[layer]))

            # Compute edge-preserving loss
            loss_edge = lambda_edge * edge_preserving_loss(target_img, fake_img)

            # Total generator loss including adversarial, L1, perceptual, and edge losses
            loss_G = loss_G_adv + loss_G_L1 + lambda_perceptual * loss_perceptual + loss_edge

            loss_G.backward()
            optimizerG.step()

            if i % 50 == 0:
                print(f"[Epoch {epoch+1}/{num_epochs}] [D loss: {loss_D.item():.4f}] [G loss: {loss_G.item():.4f}]")

        # Step the schedulers at the end of each epoch
        schedulerG.step()
        schedulerD.step()
        print(f"After Epoch {epoch+1}, Generator LR: {schedulerG.get_last_lr()[0]:.6f}, Discriminator LR: {schedulerD.get_last_lr()[0]:.6f}")

        torch.save(netG.state_dict(), f"netG_epoch_{epoch+1}.pth")
        torch.save(netD.state_dict(), f"netD_epoch_{epoch+1}.pth")

train()

def denormalize(image_tensor):
    """
    Converts an image tensor from normalized [-1, 1] space to [0, 1].
    Expects input tensor shape to be [C, H, W].
    """
    image_tensor = (image_tensor + 1) / 2.0
    image_np = image_tensor.cpu().numpy()
    image_np = np.clip(image_np, 0, 1)
    image_np = np.transpose(image_np, (1, 2, 0))
    return image_np

def generate_and_save_images(netG, dataset, device, output_folder="generated_images"):
    """
    Uses the trained generator 'netG' to generate images from the given dataset,
    then saves these images to 'output_folder' with unique filenames.
    Also computes PSNR and SSIM metrics against the provided ground-truth targets
    and displays every 10th sample for qualitative assessment.
    Finally, it shows a table with evaluation metrics for each image.
    """
    # Create the output folder if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    netG.eval()  # Set the generator to evaluation mode
    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)

    psnr_scores = []
    ssim_scores = []
    metrics_table = []  # This list will hold each image metrics as a row

    with torch.no_grad():
        for idx, (input_img, target_img) in enumerate(dataloader):
            input_img = input_img.to(device)
            target_img = target_img.to(device)

            # Generate the image using the generator
            generated_img = netG(input_img)

            # Denormalize (convert from [-1, 1] to [0, 1])
            gen_np = denormalize(generated_img.squeeze(0))
            gen_pil = Image.fromarray((gen_np * 255).astype(np.uint8))

            # Save the generated image with a unique filename
            save_path = os.path.join(output_folder, f"generated_{idx+1}.png")
            gen_pil.save(save_path)
            print(f"Saved generated image: {save_path}")

            # Denormalize target image for metric computation and visualization
            target_np = denormalize(target_img.squeeze(0))
            # Compute PSNR and SSIM between the generated image and target image
            psnr_value = psnr_metric(target_np, gen_np, data_range=1)
            ssim_value = ssim_metric(target_np, gen_np, data_range=1, channel_axis=-1)
            psnr_scores.append(psnr_value)
            ssim_scores.append(ssim_value)
            metrics_table.append([idx+1, psnr_value, ssim_value])  # Store results

            # Display every 10th sample
            if idx % 10 == 0:
                input_np = denormalize(input_img.squeeze(0))
                plt.figure(figsize=(12, 4))
                plt.subplot(1, 3, 1)
                plt.imshow(input_np)
                plt.title("Input Image")
                plt.axis('off')

                plt.subplot(1, 3, 2)
                plt.imshow(target_np)
                plt.title("Target Image")
                plt.axis('off')

                plt.subplot(1, 3, 3)
                plt.imshow(gen_np)
                plt.title(f"Generated Image\nPSNR: {psnr_value:.2f} dB\nSSIM: {ssim_value:.4f}")
                plt.axis('off')
                plt.show()

    avg_psnr = np.mean(psnr_scores)
    avg_ssim = np.mean(ssim_scores)
    print("Image generation complete.")
    print(f"Average PSNR: {avg_psnr:.2f} dB")
    print(f"Average SSIM: {avg_ssim:.4f}")

    # Create a Pandas DataFrame to show a table of the results.
    df = pd.DataFrame(metrics_table, columns=["Image Index", "PSNR (dB)", "SSIM"])
    print("\nEvaluation Metrics Table:")
    print(df.to_markdown(index=False))

# ------------------------------------------------------------------
# Setup the test dataset and load the trained generator for evaluation:
# ------------------------------------------------------------------

# Set up device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define image preprocessing
target_size = (576, 768)
transform = transforms.Compose([
    transforms.Resize(target_size),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load the test dataset
test_dataset = CustomPairedDataset(
    root_dir="/content/drive/MyDrive/reference_images/Path_2",
    transform=transform
)

# Initialize and load the generator checkpoint
netG = UNet(input_nc=3, output_nc=3).to(device)
netG.load_state_dict(torch.load("netG_epoch_10.pth", map_location=device))

# Generate images, save them, compute metrics, and visualize results.
generate_and_save_images(netG, test_dataset, device, output_folder="generated_images")

!zip -r generated_images.zip /content/generated_images/
from google.colab import files
files.download('generated_images.zip')